{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 3.1] Write Dimension Reduction Code (차원 축소)\n",
    "\n",
    "이 노트북은 pca_byoc_train.py 파일을 생성 합니다. <br>\n",
    "**이 코드는 Custome Docker Image를 생성시에 포함되어 학습/추론 코드 역할을 합니다.**<br><br>\n",
    "이 코드는 크게 두가지 목적으로 사용 됩니다. \n",
    "### 1. PCA 알고리즘 학습하여 PCA 모델 생성. 아래의 코드 블락이 그 역할을 합니다.\n",
    "```\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    ...\n",
    "    ...\n",
    "    print(\"Component Variability: \\n\", pca.explained_variance_ratio_)\n",
    "    joblib.dump(pca, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "```\n",
    "\n",
    "### 2. PCA 모델을 통한 추론을 합니다. 크게 두가지 형태의 추론이 있습니다.\n",
    "\n",
    "##### (1) 단일 모델에서 추론 하여 차원 축소\n",
    "```env={'TRANSFORM_MODE': 'feature-transform', 'LENGTH_COLS': str(num_cols)})``` 환경 변수의 'feature-transform' 값을 받은 다음에 <br>\n",
    "input_fn --> predict_fn --> output_fn 을 순차적으로 호출 하여 차원 축소 결과를 제공 합니다.\n",
    "##### (2) Inference Pipeline 사용하여 추론 통한 차원 축소\n",
    "```env={'TRANSFORM_MODE': 'inverse-label-transform'})``` 환경 변수의 'inverse-label-transform' 값을 받은 다음에 <br>\n",
    "input_fn --> predict_fn --> output_fn 을 순차적으로 호출 하여 차원 축소 결과를 제공 합니다.\n",
    "\n",
    "## 추론 Function() 상세\n",
    "##### (1) input_fn\n",
    "```\n",
    "def input_fn(input_data, request_content_type):\n",
    "```\n",
    "- 입력\n",
    "    - content-type: 'text/csv' 으로만 받습니다. 다른 타입이 오면 에러를 발생 시킵니다.\n",
    "    - data-type: String 혹은 Bytes를 받음\n",
    "- 출력\n",
    "    - pandas dataframe을 리턴\n",
    "    \n",
    "##### (2) model_fn\n",
    "```\n",
    "def model_fn(model_dir):\n",
    "    pca = joblib.load(os.path.join(model_dir, \"model.joblib\"))    \n",
    "    return pca  \n",
    "```\n",
    "- model_dir (모델의 위치)를 받아서 모델을 로딩한 후에 리턴 합니다.\n",
    "\n",
    "##### (3) predict_fn\n",
    "```\n",
    "def predict_fn(input_data, model):\n",
    "    if os.getenv('TRANSFORM_MODE') == 'feature-transform':        \n",
    "        ...\n",
    "        ...\n",
    "        features = model.transform(payload)\n",
    "        return features    \n",
    "    if os.getenv('TRANSFORM_MODE') == 'inverse-label-transform':\n",
    "        ...\n",
    "        ...\n",
    "        features = model.transform(payload)\n",
    "        return features\n",
    "\n",
    "```\n",
    "- 입력\n",
    "    - input_data: input_fn 에서 리턴한 Pandas Dataframe을 받습니다.\n",
    "    - model: model_fn에서 리턴한 model을 받습니다.\n",
    "- 로직\n",
    "    - TRANSFORM_MODE 환경 변수에 따라서, '차원 축소 배치 추론\" 를 할지, '차원 축소 실시간 추론\"을 할지를 결정하여 model.transform() 을 실행 합니다.\n",
    "- 출력\n",
    "    - Numpy array을 리턴\n",
    "\n",
    "##### (4) output_fn\n",
    "```\n",
    "def output_fn(prediction, accept):\n",
    "    ...\n",
    "    ...\n",
    "    accept = 'text/csv'\n",
    "    if accept == 'text/csv':\n",
    "        print(\"type of accept after a change : \", accept )        \n",
    "        return worker.Response(encoders.encode(prediction, accept), ```\n",
    "    ...\n",
    "```\n",
    "- 입력\n",
    "    - prediction: predict_fn 리턴한 Numpy array를 받습니다.\n",
    "    - accept: predictor.predict() 에서 제공한 accept 값을 받습니다.\n",
    "- 로직\n",
    "    - accept: 'text/csv' 로 변경 합니다.\n",
    "- 출력\n",
    "    - prdiction (numpy array), accept ('text/csv') 를 리턴 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pca_byoc_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pca_byoc_train.py\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sagemaker_containers.beta.framework import (\n",
    "    content_types, encoders, env, modules, transformer, worker)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    parser.add_argument('--n_components', type=int, default = 3)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train)]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files]\n",
    "    train_data = pd.concat(raw_data)\n",
    "    \n",
    "    pca = PCA(n_components = args.n_components)\n",
    "    print(\"train shape: \", train_data.shape)\n",
    "    X_new = pca.fit_transform(train_data)\n",
    "    \n",
    "    print(\"Component Variability: \\n\", pca.explained_variance_ratio_)\n",
    "    \n",
    "    joblib.dump(pca, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    \n",
    "def input_fn(input_data, request_content_type):\n",
    "    \"\"\"\n",
    "    기본적인 입력은 'text/csv' 를 받고, 데이타는 타입이 String 혹은 Bytes 를 받습니다.\n",
    "    Parse input data payload    \n",
    "    We currently only take csv input. \n",
    "    \"\"\"\n",
    "  \n",
    "    print(\"### input_fn: Starting ###\")\n",
    "    print(\"type of input_data: \", type(input_data))\n",
    "    print(\"request_content_type: \", request_content_type)\n",
    "        \n",
    "    content_type = request_content_type.lower(\n",
    "    ) if request_content_type else \"text/csv\"\n",
    "    content_type = content_type.split(\";\")[0].strip()\n",
    "        \n",
    "    \n",
    "    if isinstance(input_data, str):\n",
    "        str_buffer = input_data\n",
    "    else:\n",
    "        # In the case of getting byte array like b'hello'        \n",
    "        str_buffer = str(input_data,'utf-8') \n",
    "\n",
    "    \n",
    "    if (content_type == 'text/csv' or content_type == 'text/csv; charset=utf-8'):\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(str_buffer),  header=None)\n",
    "        logging.info(f\"input_fn: \")      \n",
    "        logging.info(f\"shape of requested data: '{df.shape}'\")        \n",
    "        logging.info(f\"requested data: '{df}'\")\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))    \n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialized and return fitted model\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"   \n",
    "    pca = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    \n",
    "    return pca  \n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "    \n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    print(\"prdict_fn: Staring\")\n",
    "    print(\"os.getenv('TRANSFORM_MODE'): \", os.getenv('TRANSFORM_MODE'))    \n",
    "\n",
    "    \n",
    "        \n",
    "    if os.getenv('TRANSFORM_MODE') == 'feature-transform':        \n",
    "\n",
    "        import numpy as np\n",
    "        print(\"predcit_fn - TRANSFORM_MODE: \", os.getenv('TRANSFORM_MODE'))\n",
    "        logging.info(f\"predict_fn: input_data - '{input_data}'\")\n",
    "        # model, PCA model, has transform()\n",
    "        print(\"type of input_data: \", type(input_data))\n",
    "        print(\"shape of input_data: \", input_data.shape)        \n",
    "        print(\"head of input_data: \\n \", input_data[0:2])  \n",
    "        payload = input_data.iloc[:,1:] # Exclude a label\n",
    "\n",
    "        print(\"os.getenv('LENGTH_COLS') : \", os.getenv('LENGTH_COLS'))\n",
    "        print(\"type: os.getenv('LENGTH_COLS'): \", type(os.getenv('LENGTH_COLS')))\n",
    "        num_cols = int(os.getenv('LENGTH_COLS')) - 1 # exclude a label\n",
    "\n",
    "        payload = payload.values.reshape(-1,num_cols)\n",
    "        components = model.transform(payload)\n",
    "\n",
    "        print(\"type of components: \", type(components))\n",
    "        print(\"shape of components: \", components.shape)\n",
    "\n",
    "        # Add label column to the front\n",
    "        features = np.insert(components, 0, input_data.iloc[:,0].values, axis=1)\n",
    "\n",
    "        logging.info(f\"predict_fn: PCA components: \\n'{features}'\")    \n",
    "\n",
    "        return features\n",
    "    \n",
    "        \n",
    "    # In th ecase of not being set to env. variable        \n",
    "    if os.getenv('TRANSFORM_MODE') == 'inverse-label-transform':\n",
    "        print(\"predcit_fn - TRANSFORM_MODE: \", os.getenv('TRANSFORM_MODE'))\n",
    "        # model, PCA model, has transform()\n",
    "        print(\"type of input_data: \", type(input_data))\n",
    "        print(\"shape of input_data: \", input_data.shape)        \n",
    "        print(\"head of input_data: \\n \", input_data[0:2])  \n",
    "        \n",
    "        payload = input_data\n",
    "\n",
    "        # num_cols = int(os.getenv('LENGTH_COLS'))\n",
    "        num_cols = 69\n",
    "\n",
    "        payload = payload.values.reshape(-1,num_cols)\n",
    "        features = model.transform(payload)\n",
    "\n",
    "        logging.info(f\"predict_fn: PCA components: \\n'{features}'\")    \n",
    "\n",
    "        return features\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "    \n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    logging.info(f\"Output_fn: prdiction - '{prediction}' \")                \n",
    "    print(\"Output_fn-type of accept : \", accept )\n",
    "    \n",
    "    accept = 'text/csv'\n",
    "    if type(prediction) is not np.ndarray:\n",
    "        prediction=prediction.toarray()\n",
    "    \n",
    "    print(\"output_fn-type of prediction: \", type(prediction))    \n",
    "   \n",
    "    if accept == 'text/csv':\n",
    "        print(\"type of accept after a change : \", accept )        \n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n",
    "        \n",
    "    \n",
    "# predict_fn을 정의하지 않으면 default predict_fn을 호출 함.\n",
    "# PCA는 predict 함수를 제공하지 않으므로 사용자 정의 필요 함.\n",
    "\n",
    "# algo-1-dhteh_1  | 2020-08-10 14:15:55,970 ERROR - pca_train - Exception on /invocations [POST]\n",
    "# algo-1-dhteh_1  | Traceback (most recent call last):\n",
    "# algo-1-dhteh_1  |   File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_functions.py\", line 93, in wrapper\n",
    "# algo-1-dhteh_1  |     return fn(*args, **kwargs)\n",
    "# algo-1-dhteh_1  |   File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py\", line 70, in default_predict_fn\n",
    "# algo-1-dhteh_1  |     output = model.predict(input_data)\n",
    "# algo-1-dhteh_1  | AttributeError: 'PCA' object has no attribute 'predict'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
