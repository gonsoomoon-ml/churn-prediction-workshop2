{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Script-Mode Custom Training Container</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile pca_script_train.py\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "# import argparse\n",
    "# import joblib\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def _is_inverse_label_transform():\n",
    "#     \"\"\"Returns True if if it's running in inverse label transform.\"\"\"\n",
    "#     return os.getenv('TRANSFORM_MODE') == 'inverse-label-transform'\n",
    "\n",
    "# def _is_feature_transform():\n",
    "#     \"\"\"Returns True if it's running in feature transform mode.\"\"\"\n",
    "#     return os.getenv('TRANSFORM_MODE') == 'feature-transform'\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "#     parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "#     parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "#     parser.add_argument('--n_components', type=int, default = 3)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train)]\n",
    "#     if len(input_files) == 0:\n",
    "#         raise ValueError(('There are no files in {}.\\n' +\n",
    "#                           'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "#                           'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "#                           'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "#     raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files]\n",
    "#     train_data = pd.concat(raw_data)\n",
    "    \n",
    "#     pca = PCA(n_components = args.n_components)\n",
    "#     print(\"train shape: \", train_data.shape)\n",
    "#     X_new = pca.fit_transform(train_data)\n",
    "    \n",
    "#     print(\"Component Variability: \\n\", pca.explained_variance_ratio_)\n",
    "    \n",
    "#     joblib.dump(pca, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    \n",
    "\n",
    "# def model_fn(model_dir):\n",
    "#     \"\"\"\n",
    "#     Deserialized and return fitted model\n",
    "#     Note that this should have the same name as the serialized model in the main method\n",
    "#     \"\"\"   \n",
    "#     pca = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    \n",
    "#     return pca  \n",
    "\n",
    "# def predict_fn(input_data, model):\n",
    "#     \"\"\"Preprocess input data\n",
    "    \n",
    "#     We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "#     so we want to use .transform().\n",
    "#     \"\"\"\n",
    "#     import os\n",
    "#     import pprint\n",
    "    \n",
    "#     env_var = os.environ \n",
    "#     print(\"################ Environment Variables: ################\") \n",
    "#     pprint.pprint(dict(env_var), width = 1)     \n",
    "    \n",
    "# #     if _is__feature_transform():\n",
    "#     print(\"TRANSFORM_MODE: \", os.getenv('TRANSFORM_MODE'))\n",
    "#     logging.info(f\"predict_fn: input_data - '{input_data}'\")\n",
    "#     # model, PCA model, has transform()\n",
    "#     print(\"type of input_data: \", type(input_data))\n",
    "#     print(\"shape of input_data: \", input_data.shape)        \n",
    "#     print(\"head of input_data: \\n \", input_data.head(2))                \n",
    "#     components = model.transform(input_data)\n",
    "\n",
    "#     logging.info(f\"predict_fn: PCA components: \\n'{components}'\")    \n",
    "#     return components\n",
    "    \n",
    "# # predict_fn을 정의하지 않으면 default predict_fn을 호출 함.\n",
    "# # PCA는 predict 함수를 제공하지 않으므로 사용자 정의 필요 함.\n",
    "\n",
    "# # algo-1-dhteh_1  | 2020-08-10 14:15:55,970 ERROR - pca_train - Exception on /invocations [POST]\n",
    "# # algo-1-dhteh_1  | Traceback (most recent call last):\n",
    "# # algo-1-dhteh_1  |   File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_functions.py\", line 93, in wrapper\n",
    "# # algo-1-dhteh_1  |     return fn(*args, **kwargs)\n",
    "# # algo-1-dhteh_1  |   File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py\", line 70, in default_predict_fn\n",
    "# # algo-1-dhteh_1  |     output = model.predict(input_data)\n",
    "# # algo-1-dhteh_1  | AttributeError: 'PCA' object has no attribute 'predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pca_byoc_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pca_byoc_train.py\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    parser.add_argument('--n_components', type=int, default = 3)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train)]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files]\n",
    "    train_data = pd.concat(raw_data)\n",
    "    \n",
    "    pca = PCA(n_components = args.n_components)\n",
    "    print(\"train shape: \", train_data.shape)\n",
    "    X_new = pca.fit_transform(train_data)\n",
    "    \n",
    "    print(\"Component Variability: \\n\", pca.explained_variance_ratio_)\n",
    "    \n",
    "    joblib.dump(pca, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    \n",
    "def input_fn(input_data, request_content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "    \n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "        \n",
    "    content_type = request_content_type.lower(\n",
    "    ) if request_content_type else \"text/csv\"\n",
    "    content_type = content_type.split(\";\")[0].strip()\n",
    "        \n",
    "    \n",
    "    if isinstance(input_data, str):\n",
    "        str_buffer = input_data\n",
    "    else:\n",
    "        # str_buffer = str(input_data,'utf-8')\n",
    "        str_buffer = input_data\n",
    "    \n",
    "    if (content_type == 'text/csv' or content_type == 'text/csv; charset=utf-8'):\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(str_buffer),  header=None)\n",
    "        logging.info(f\"input_fn: \")      \n",
    "        logging.info(f\"shape of requested data: '{df.shape}'\")        \n",
    "        logging.info(f\"requested data: '{df}'\")\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))    \n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialized and return fitted model\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"   \n",
    "    pca = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    \n",
    "    return pca  \n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "    \n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    print(\"prdict_fn: Staring\")\n",
    "    print(\"os.getenv('TRANSFORM_MODE'): \", os.getenv('TRANSFORM_MODE'))    \n",
    "\n",
    "    \n",
    "        \n",
    "    if os.getenv('TRANSFORM_MODE') == 'feature-transform':        \n",
    "\n",
    "        import numpy as np\n",
    "        print(\"predcit_fn - TRANSFORM_MODE: \", os.getenv('TRANSFORM_MODE'))\n",
    "        logging.info(f\"predict_fn: input_data - '{input_data}'\")\n",
    "        # model, PCA model, has transform()\n",
    "        print(\"type of input_data: \", type(input_data))\n",
    "        print(\"shape of input_data: \", input_data.shape)        \n",
    "        print(\"head of input_data: \\n \", input_data[0:2])  \n",
    "        payload = input_data.iloc[:,1:] # Exclude a label\n",
    "\n",
    "        print(\"os.getenv('LENGTH_COLS') : \", os.getenv('LENGTH_COLS'))\n",
    "        print(\"type: os.getenv('LENGTH_COLS'): \", type(os.getenv('LENGTH_COLS')))\n",
    "        num_cols = int(os.getenv('LENGTH_COLS')) - 1 # exclude a label\n",
    "\n",
    "        payload = payload.values.reshape(-1,num_cols)\n",
    "        components = model.transform(payload)\n",
    "\n",
    "        print(\"type of components: \", type(components))\n",
    "        print(\"shape of components: \", components.shape)\n",
    "\n",
    "        # Add label column to the front\n",
    "        features = np.insert(components, 0, input_data.iloc[:,0].values, axis=1)\n",
    "\n",
    "        logging.info(f\"predict_fn: PCA components: \\n'{features}'\")    \n",
    "\n",
    "        return features\n",
    "    \n",
    "    if os.getenv('TRANSFORM_MODE') == 'inverse-label-transform':\n",
    "        print(\"predcit_fn - TRANSFORM_MODE: \", os.getenv('TRANSFORM_MODE'))\n",
    "        # model, PCA model, has transform()\n",
    "        print(\"type of input_data: \", type(input_data))\n",
    "        print(\"shape of input_data: \", input_data.shape)        \n",
    "        print(\"head of input_data: \\n \", input_data[0:2])  \n",
    "        \n",
    "        payload = input_data\n",
    "\n",
    "        num_cols = int(os.getenv('LENGTH_COLS'))\n",
    "\n",
    "        payload = payload.values.reshape(-1,num_cols)\n",
    "        features = model.transform(payload)\n",
    "\n",
    "        logging.info(f\"predict_fn: PCA components: \\n'{features}'\")    \n",
    "\n",
    "        return features\n",
    "        \n",
    "    # In the case of not being set to env. variable        \n",
    "    if os.getenv('TRANSFORM_MODE') == None:\n",
    "        print(\"predcit_fn - TRANSFORM_MODE: \", os.getenv('TRANSFORM_MODE'))\n",
    "        # model, PCA model, has transform()\n",
    "        print(\"type of input_data: \", type(input_data))\n",
    "        print(\"shape of input_data: \", input_data.shape)        \n",
    "        print(\"head of input_data: \\n \", input_data[0:2])  \n",
    "        \n",
    "        payload = input_data\n",
    "\n",
    "        # num_cols = int(os.getenv('LENGTH_COLS'))\n",
    "        num_cols = 69\n",
    "\n",
    "        payload = payload.values.reshape(-1,num_cols)\n",
    "        features = model.transform(payload)\n",
    "\n",
    "        logging.info(f\"predict_fn: PCA components: \\n'{features}'\")    \n",
    "\n",
    "        return features\n",
    "\n",
    "        \n",
    "    \n",
    "# predict_fn을 정의하지 않으면 default predict_fn을 호출 함.\n",
    "# PCA는 predict 함수를 제공하지 않으므로 사용자 정의 필요 함.\n",
    "\n",
    "# algo-1-dhteh_1  | 2020-08-10 14:15:55,970 ERROR - pca_train - Exception on /invocations [POST]\n",
    "# algo-1-dhteh_1  | Traceback (most recent call last):\n",
    "# algo-1-dhteh_1  |   File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_functions.py\", line 93, in wrapper\n",
    "# algo-1-dhteh_1  |     return fn(*args, **kwargs)\n",
    "# algo-1-dhteh_1  |   File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py\", line 70, in default_predict_fn\n",
    "# algo-1-dhteh_1  |     output = model.predict(input_data)\n",
    "# algo-1-dhteh_1  | AttributeError: 'PCA' object has no attribute 'predict'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
